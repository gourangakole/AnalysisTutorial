---+ Analysis Tutorial

---++ Overview

The main goal of this twiki is to document a series of training sessions to teach the basics of doing a particle physics analysis from a practical perspective. The loose set of topics that will be covered are:

   * ROOT -- the C++ based data analysis package that is used in HEP
   * !CMSSW -- the software framework used by the collaboration
   * CMS2 -- the software sub-framework used by the UCSD/UCSB/FNAL group (a.k.a. SNT)
   * A full analysis example -- measuring the Z cross section

These topics are not necessarily ordered in any particular way and are only loosely related.

%TOC{title="Contents:"}%

---++ Order of topics (Subject to change)
---+++ ROOT
   * Documentation 
      * Home: http://root.cern.ch/drupal/
      * User's guide: http://root.cern.ch/drupal/content/users-guide
      * Classes: http://root.cern.ch/root/html534/ClassIndex.html
      * Tutorial Example Code: http://root.cern.ch/root/html/tutorials/
   * To Check out the example code:
<pre>git clone <a target="_blank" href="https://github.com/kelleyrw/AnalysisTutorial" style="color: #1155cc; font-family: arial, sans-serif; font-size: 13px; background-color: #ffffff">https://github.com/kelleyrw/AnalysisTutorial</a></pre> 
---++++ Lesson 1
   * Studied the basics of TTree and made efficiency plots for some tracking variables
   * Reading: ROOT user's guide: read ch 1-3,7,12
   * Example code: <a target="_blank" href="https://github.com/kelleyrw/AnalysisTutorial/tree/master/lesson1">Lesson 1</a>
---+++++ TTree example
 To facilitate a non-trivial example of making plots, a very simple TTree was constructed using CMSSW that contains the composite generated/simulated particle known as [[https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideTrackingTruth][tracking particles]]. You can think of these tracking particles as the combined generator and simulated truth information of all the debris of p-p collision (e.g. Pythia6 status 1). These tracking particles are associated with reconstructed tracks by looking at the simulated energy deposits in the tracker (sim hits) and matching them to the reconstructed hits from the track reconstruction algorithms (rec hits). We will go into how this TTree was made in a later lesson.

This tree was filled per event and contains a unfiltered list (in the form of a std::vector) of !TrackingParticles per event:

<verbatim>
Events
  |
  --> list of TrackingParticles
             |
             --> Tracking particle information (p4, # sim hits, d0, dz, ...) 
             --> Matching reconstructed Track info (bogus values filled if no matching track).
</verbatim>

The tree is small (1000 events) and I was able to check into the repository (https://github.com/kelleyrw/AnalysisTutorial/blob/master/week1/trees/tracking_ntuple.root). All the branches should be the same size:

<verbatim>
// TrakingParticle info
std::vector<LorentzVector> tps_p4:  four momentum
std::vector<int> tps_pdgid:         pdg particle ID code: http://pdg.lbl.gov/2007/reviews/montecarlorpp.pdf
std::vector<double> tps_d0:         transverse impact parameter
std::vector<double> tps_dz:         longitudinal impact parameter
std::vector<bool> tps_matched:      matched to track?  true/false 
std::vector<int> tps_charge:        charge
std::vector<int> tps_nhits:         # of simulated hits

// reco track info
std::vector<LorentzVector> trks_p4:  four momentum
std::vector<double> trks_tip:        transverse impact parameter  (from the TrackingParticle vertex)
std::vector<double> trks_lip:        longitudinal impact parameter  (from the TrackingParticle vertex)
std::vector<double> trks_d0:         transverse impact parameter (using the trajectory builder)
std::vector<double> trks_dz:         longitudinal impact parameter (using the trajectory builder)
std::vector<double> trks_pterr:      pt uncertainty
std::vector<double> trks_d0err:      d0 uncertainty
std::vector<double> trks_dzerr:      dz uncertainty
std::vector<double> trks_chi2:       chi^2 of the track's fit
std::vector<int> trks_ndof:          # degrees of freedom
std::vector<int> trks_nlayers:       # number of valid layers with a measurement
std::vector<bool> trks_high_purity:  # track passes high purity quality requirement
</verbatim>

---+++++ Playing with TTree::Draw

TTree::Draw gives you basic plotting from the ROOT prompt. This is convenient for quick studies and on the fly plot making. In addition to the chapter 12 on TTree from the ROOT User's guide, you should also read the documentation in the class webpage: http://root.cern.ch/root/html532/TTree.html#TTree:Draw%2

Open the ROOT file and try playing with the following examples:

<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson1]$  root ../data/tracking_ntuple.root 
root [0] 
Attaching file ../data/tracking_ntuple.root as _file0...
root [1] .ls
</verbatim>

List the branches in the tree:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root [2] tree->Print()
</verbatim>

List the branches in the tree with a less verbose printout:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root [3] tree->GetListOfBranches()->ls()
</verbatim>

Draw the tracking particles's pT.
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root [4] tree->Draw("tps_p4.pt()") 
</verbatim> <img width="596" alt="tps_p4_pt_ex1.png" src="%ATTACHURLPATH%/tps_p4_pt_ex1.png" height="572" />

On the previous plot, the automatic binning choice was sub optimal since it tries to get everything included in a bin. To specific the binning explicitly:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root [5] tree->Draw("tps_p4.pt()>>(100,0,10)")
</verbatim> <img width="596" alt="tps_p4_pt_ex2.png" src="%ATTACHURLPATH%/tps_p4_pt_ex2.png" height="572" />

In order to keep have a handle to the histogram for later manipulation, you can name the output hist. Now you can do subsequent operations on it.
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root [7] tree->Draw("tps_p4.pt()>>h1(100,0,10)")
root [8] h1->SetLineColor(kRed)
root [9] h1->SetTitle("tracking particle p_{T};p_{T} (GeV);A.U.")
root [10] h1->SetLineWidth(2)
root [11] h1->Draw()
</verbatim> <img width="596" alt="tps_p4_pt_ex3.png" src="%ATTACHURLPATH%/tps_p4_pt_ex3.png" height="572" />

To make a selection, use the 2nd field. This is also an example of how to overlay to plots.
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root [12] tree->Draw("tps_p4.pt()>>h_pt_barrel(100,0,10)", "fabs(tps_p4.eta())<1.1");
root [13] h_pt_barrel->Draw();
root [14] h_pt_barrel->SetLineColor(kBlue);
root [15] h_pt_barrel->SetLineWidth(2);
root [16] h_pt_barrel->SetTitle("Tracking Particle p_{T} (Barrel);p_{T} (GeV)");
root [17] h_pt_barrel->Draw();
root [18] tree->Draw("tps_p4.pt()>>h_pt_endcap(100,0,10)", "fabs(tps_p4.eta())>1.1");
root [19] h_pt_endcap->SetLineColor(kRed);
root [20] h_pt_endcap->SetLineWidth(2);
root [21] h_pt_endcap->SetTitle("Tracking Particle p_{T} (Endcap);p_{T} (GeV)");
root [22] h_pt_endcap->Draw();
root [23] h_pt_barrel->Draw("sames");
root [24] TLegend leg(0.3, 0.8, 0.6, 0.5);
root [25] leg.AddEntry(h_pt_endcap, "Endcap");
root [26] leg.AddEntry(h_pt_barrel, "Barrel");
root [27] leg.SetFillStyle(0);
root [28] leg.Draw();
</verbatim> <img width="596" alt="tps_p4_pt_overlay.png" src="%ATTACHURLPATH%/tps_p4_pt_overlay.png" height="572" />

Now at this point, you may be sick of typing in commands everytime. Time to use a macro (see chapter 7). Consider the following macro (https://github.com/kelleyrw/AnalysisTutorial/tree/master/lesson1/macros/overlay.C) which is the same as the previous example
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
{
    tree->Draw("tps_p4.pt()>>h_pt_barrel(100,0,10)", "fabs(tps_p4.eta())<1.1");
    h_pt_barrel->Draw();
    h_pt_barrel->SetLineColor(kBlue);
    h_pt_barrel->SetLineWidth(2);
    h_pt_barrel->SetTitle("Tracking Particle p_{T} (Barrel);p_{T} (GeV)");
    h_pt_barrel->Draw();

    tree->Draw("tps_p4.pt()>>h_pt_endcap(100,0,10)", "fabs(tps_p4.eta())>1.1");
    h_pt_endcap->SetLineColor(kRed);
    h_pt_endcap->SetLineWidth(2);
    h_pt_endcap->SetTitle("Tracking Particle p_{T} (Endcap);p_{T} (GeV)");

    h_pt_endcap->Draw();
    h_pt_barrel->Draw("sames");
    TLegend leg(0.3, 0.8, 0.6, 0.5);
    leg.AddEntry(h_pt_endcap, "Endcap");
    leg.AddEntry(h_pt_barrel, "Barrel");
    leg.SetFillStyle(0);
    leg.Draw();
}
</verbatim>

To run, you open the ROOT tree and the run it:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson1]$  root ../data/tracking_ntuple.root 
root [0] 
Attaching file ../data/tracking_ntuple.root as _file0...
root [1] .x macros/overlay.C 
Info in <TCanvas::MakeDefCanvas>:  created default TCanvas with name c1
</verbatim>

---+++++ Efficiency Plots

Our next example is to use this simple tree to make an efficiency plot vs eta. We define the efficiency is a ratio:

&epsilon; = numerator count / denominator count

Where denominator is a tracking particle that has a reasonable chance of actually being reconstructed:
   * non zero charge
   * pT &gt; 0.9 !GeV
   * |&eta;| &lt; 2.5
   * |transverse impact parameter| &lt; 3.5 cm
   * |longitudinal impact parameter| &lt; 30 cm
   * |# of simulated hits| &gt;= 3

The numerator selection is the same as the denominator selection except that we require the tracking particle to be matched to a reconstructed track.

The following macro produces this plot: https://github.com/kelleyrw/AnalysisTutorial/tree/master/lesson1/macros/eff.C
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson1]$  root ../data/tracking_ntuple.root 
root [0] 
Attaching file ../data/tracking_ntuple.root as _file0...
root [1] .x macros/eff.C 
Info in <TCanvas::MakeDefCanvas>:  created default TCanvas with name c1
</verbatim> &lt;/verbatim&gt; <img width="596" alt="eff_vs_eta.png" src="%ATTACHURLPATH%/eff_vs_eta.png" height="572" />

As an exercise we're going to make this same plot two more times. The next example, we're going to compile the macro. In general it is a good idea to compile you macros because the interpreter (CINT) is not robust and can incorrectly interpret even simple code. Also, it will greatly increase the execution time if the macro is doing anything significant. See chapter 7 of the User's Guide for more details. The following macro produces this plot same plot as above but is meant to run compiled: https://github.com/kelleyrw/AnalysisTutorial/tree/master/lesson1/macros/eff_compiled.C

<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson1]$  root
root [0] .L macros/eff_compiled.C++
Info in <TUnixSystem::ACLiC>: creating shared library /Users/rwk7t/temp/.rootbuild//Users/rwk7t/Development/newbies/lesson1/./macros/eff_compiled_C.so
root [1] eff_compiled("../data/tracking_ntuple.root", "plots/eff_vs_eta.root", "png")
[eff_compiled] tree is opened with 1000 entries
[eff_compiled] printing plots to plots directory.
Info in <TCanvas::Print>: png file plots/h_num_vs_eta_compiled.png has been created
Info in <TCanvas::Print>: png file plots/h_den_vs_eta_compiled.png has been created
Info in <TCanvas::Print>: png file plots/h_eff_vs_eta_compiled.png has been created
[eff_compiled] writing plots to plots/eff_vs_eta.root
</verbatim> &lt;/verbatim&gt; <img width="596" alt="h_num_vs_eta_compiled.png" src="%ATTACHURLPATH%/h_num_vs_eta_compiled.png" height="572" /> &lt;/verbatim&gt; <img width="596" alt="h_den_vs_eta_compiled.png" src="%ATTACHURLPATH%/h_den_vs_eta_compiled.png" height="572" /> &lt;/verbatim&gt; <img width="596" alt="h_eff_vs_eta_compiled.png" src="%ATTACHURLPATH%/h_eff_vs_eta_compiled.png" height="572" />

The final example from this lesson is a very simple "looper". A looper is simple a macro that manually loops over the entries in a TTree rather than relying on TTree::Draw(). This has the advantage of speed since there will be only one loop over the tree instead of one for each plot. Also, it has the flexibility to implement arbitrary logic whereas TTree::Draw, while flexible, can still be limited on what you can calculate and plot.

The following macro is a simple looper: https://github.com/kelleyrw/AnalysisTutorial/tree/master/lesson1/macros/eff_looper.C. It breaks up the process into two steps: create the numerator and denominator plots (!CreatePlots) and doing the division to make the efficiency (!FinalPlots). The purpose of this is making the numerator/denominator is the "slow" part -- you don't want to remake these plots if you all you need is to change the label or something on the efficiency plot. This is a simple example of breaking up the work flow to keep the analysis running efficiently. You can, of course, make a function that simply calls both if you want the option of doing it in two steps (exercise left to the reader)
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root [2] .L macros/eff_looper.C++
Info in <TUnixSystem::ACLiC>: creating shared library /Users/rwk7t/temp/.rootbuild//Users/rwk7t/Development/newbies/lesson1/./macros/eff_looper_C.so
root [3] CreatePlots("../data/tracking_ntuple.root", "plots/counts_vs_eta_looper.root")
root [4] FinalPlots("plots/counts_vs_eta_looper.root", "plots/eff_vs_eta_looper.root", "png")
Info in <TCanvas::Print>: png file plots/h_num_vs_eta_looper.png has been created
Info in <TCanvas::Print>: png file plots/h_den_vs_eta_looper.png has been created
Info in <TCanvas::Print>: png file plots/h_eff_vs_eta_looper.png has been created
</verbatim> &lt;/verbatim&gt; <img width="596" alt="h_num_vs_eta_looper.png" src="%ATTACHURLPATH%/h_num_vs_eta_looper.png" height="572" /> &lt;/verbatim&gt; <img width="596" alt="h_den_vs_eta_looper.png" src="%ATTACHURLPATH%/h_den_vs_eta_looper.png" height="572" /> &lt;/verbatim&gt; <img width="596" alt="h_eff_vs_eta_looper.png" src="%ATTACHURLPATH%/h_eff_vs_eta_looper.png" height="572" />

---+++++ Homework
   * check out, understand, and run the macros from lesson1
   * modify the macros to also produce the efficiency vs pt. The only change to the selection is since this is an efficiency vs pT, we should reduce the pT threshold to pT &gt; 0.1 !GeV.
   * Create resolution plots vs eta and pT. They are made by x_{reco} - x_{truth}. Do this using whatever technique you feel is appropriate. These should looks similar (but not exactly) to: 
      * http://cmsdoc.cern.ch/cms/Physics/tracking/validation/MC/CMSSW_6_1_0/START61_V8_noPU_ootb/RelValTTbar/resolutionsEta.pdf * http://cmsdoc.cern.ch/cms/Physics/tracking/validation/MC/CMSSW_6_1_0/START61_V8_noPU_ootb/RelValTTbar/resolutionsPt.pdf
   * write a brief discuss the features you see in the plots and try to explain them based on what we've gone over in the student given lectures

---++++ Lesson 2
 This lesson is an example skeleton for organizing an analysis. This is a short lesson and the main point of this lesson is to show how to organize an analysis in a more structured way and to introduce some of the abstractions that will lead us to the conventions of CMSSW. 
   * <a target="_blank" href="https://github.com/kelleyrw/AnalysisTutorial/tree/master/lesson2">Lesson 2 example code</a>
   * This example uses the same simple tracking tree from the previous lesson and also produces the same plots to show, what I consider, a less tedious way. There will always be an overhead to making plots and doing an analysis; however, with some smart choices it can be less tedious.

---+++++ Code Layout
   * In the previous example looper (eff_looper.C), everything was contained in the same file. Here the code will be re-factored and extended to promote some code re-use. Below is a list of all the files and directories

<verbatim>
|____compile.C                              File to compile the code using ACLiC
|____compile.sh                             File to compile the code using straight up gcc
|____include                              Directory to include header files for reusable code
| |____HistTools.h                        Header file for example ROOT histogram tools
| |____TRKEFF.h                           Header file for the TTree branch wrapper class
|____lib                                  Directory to contain the libraries
|____macros                               Directory to contain ACLiC compiled simple macros (usually throw away style code)
| |____FormattedPlots.C                   Simple macro to print the formated plots
| |____run_all.C                          Simple macro to run the analysis
|____plots                                Directory to contain the output of the plots
|____source                               Directory to contain the main source code for the analysis
| |____HistTools.cc                       Source file for the example ROOT histogram tools
| |____TrackingEfficiencyAnalysis.cc      Source file for the main analysis looper
| |____TRKEFF.cc                          Source file for the TTree branch wrapper class
</verbatim>

   * Note: that I'm using the CMSSW convention that *.C files are intended to be ROOT compiled macros and *.cc files are supposed to be "fully C++" complaint code.
The main analysis is done in [[https://github.com/kelleyrw/AnalysisTutorial/blob/master/lesson2/source/TrackingEfficiencyAnalysis.cc][TrackingEfficiencyAnalysis.cc]]. This is a C++ class that holds all of the metadata and runs the analysis. The main reason to make this a class is to keep all of the relevant variables and data together. If this were a set of functions, you would have to pass a bunch of parameters back and forth -- a class is more more suited for this purpose. Also, In the class definition below <verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
class TrackingEfficiencyAnalysis
{
    public:
        // construct: 
        TrackingEfficiencyAnalysis
        (
             const std::string& output_file_name = "plots/counts_vs_eta_looper.root",
             const std::string& suffix = "png",
             const bool verbose = true
        );

        // destroy:
        ~TrackingEfficiencyAnalysis();

        // method to run on the data:
        void ScanChain(TChain& chain, long long num_events = std::numeric_limits<long long>::max());

    private:
        // analysis methods:
        void BeginJob();
        void EndJob();
        void Analyze();

        // members:
        const std::string m_output_file_name;
        const std::string m_suffix;
        const bool m_verbose;
        TH1Map m_hist_map;
};
</verbatim>

the analysis methods were deliberately chosen. You will see this methodology again when we deal with CMSSW.
   * The [[https://github.com/kelleyrw/AnalysisTutorial/blob/master/lesson2/source/TrackingEfficiencyAnalysis.cc#L90][TrackingEfficiencyAnalysis::BeginJob]] function is to be called before the analysis loop begins. It can be used to setup the initial conditions for the analysis (open files, book histograms, set flags, etc.).
   * The [[https://github.com/kelleyrw/AnalysisTutorial/blob/master/lesson2/source/TrackingEfficiencyAnalysis.cc#L197][TrackingEfficiencyAnalysis::Analyze]] function is the main logic to be performed on an individual event.
   * The [[https://github.com/kelleyrw/AnalysisTutorial/blob/master/lesson2/source/TrackingEfficiencyAnalysis.cc#L143][TrackingEfficiencyAnalysis::EndJob]] function is to be called after the analysis loop ends. It can be used to save the results for the analysis (close files, save and histograms, print tables, etc.).
   * The [[https://github.com/kelleyrw/AnalysisTutorial/blob/master/lesson2/source/TrackingEfficiencyAnalysis.cc#L317][TrackingEfficiencyAnalysis::ScanChain]] function is the main looper. Before it loops, it calls the !BeginJob() function. It then loops over the whole TChain and calls the Analyze() method for each entry in the tree. Finally, after the loop it calls the !EndJob() method.

---+++++ [[https://github.com/kelleyrw/AnalysisTutorial/blob/master/lesson2/include/HistTools.h][HistTools]] -- a starter kit for histogram tools

In order to have some re-usable ROOT tools, I've provided a "starter" set for people. I have a much more mature version [[https://github.com/kelleyrw/AnalysisTools][here]] but it may not be a bad idea to start doing some of these yourself to familiarize yourself with ROOT. When you get comfortable you should feel free to rip off from my area on anyone else since the main goal is analysis not tools. HistTools provides a very basic set of histogram tools.

---+++++ [[https://github.com/kelleyrw/AnalysisTutorial/blob/master/lesson2/include/TRKEFF.h][TRKEFF]] class

This a piece of code that was automatically generated to give wrapper functions to the branches of the this TTree. It only works on the tree in this example and if the tree itself is changes, then it will need to be regenerated. The main point is twofold:
   1 It provides a simple interface to access the branches of the tree in a looper without having to call the boiler plate functions everytime (see previous lessons eff_looper.C).
   1 It provides "lazy evaluation" on the branches. By default, none of the branches are loaded. In your analysis logic, it loads the branch the first time you access it then leaves it initialized until you access the next entry in the tree (event).

We will discuss this more when we talk about CMS2 in latter lessons.

---+++++ Building and Running the code
 I've provided two ways to build the code. The first is using ROOT wrapper to gcc called [[http://root.cern.ch/download/doc/ROOTUsersGuideHTML/ch07s09.html][ACLiC]] (the .L macros.C++ thing). The second example is with GCC directly to show you what is under the hood. There is no reason you have to stick with these two methods and they have their pluses and minuses; however, this is all that is really needed to get started on analysis.

---++++++ !ACLiC
 I've provided a simple macro to compile the code called [[https://github.com/kelleyrw/AnalysisTutorial/blob/master/lesson2/compile.C][compile.C]]. To compile this analysis code, you simple run the macro in ROOT: <verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root [0] .x compile.C+
Info in <TUnixSystem::ACLiC>: creating shared library /Users/rwk7t/temp/.rootbuild//Users/rwk7t/Development/newbies/lesson2/./compile_C.so
Info in <TUnixSystem::ACLiC>: creating shared library /Users/rwk7t/Development/newbies/lesson2/lib/libHistTools.so
Info in <TUnixSystem::ACLiC>: creating shared library /Users/rwk7t/Development/newbies/lesson2/lib/libTRKEFF.so
Info in <TUnixSystem::ACLiC>: creating shared library /Users/rwk7t/Development/newbies/lesson2/lib/libTrackingEfficiencyAnalysis.so
(bool)1
</verbatim>

When you ready to run the code, there is a simple wrapper to compile the code, create an !TrackingEfficiencyAnalysis object, and run it called [[https://github.com/kelleyrw/AnalysisTutorial/blob/master/lesson2/macros/run_all.C][run_all.C]]:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root [0] .x macros/run_all.C 
Info in <TUnixSystem::ACLiC>: creating shared library /Users/rwk7t/temp/.rootbuild//Users/rwk7t/Development/newbies/lesson2/./compile_C.so
[TrackingEfficiencyAnalysis::ScanChain] finished processing 1000 events
------------------------------
CPU  Time: 1.2
Real Time: 1.2

[TrackingEfficiencyAnalysis::EndJob] printing histograms:
Info in <TCanvas::Print>: png file plots/h_cotthetares_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_cotthetares_vs_eta_sigma.png has been created
Info in <TCanvas::Print>: png file plots/h_d0res_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_d0res_vs_eta_sigma.png has been created
Info in <TCanvas::Print>: png file plots/h_den_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_den_vs_pt.png has been created
Info in <TCanvas::Print>: png file plots/h_dzres_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_dzres_vs_eta_sigma.png has been created
Info in <TCanvas::Print>: png file plots/h_eff_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_eff_vs_pt.png has been created
Info in <TCanvas::Print>: png file plots/h_num_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_num_vs_pt.png has been created
Info in <TCanvas::Print>: png file plots/h_phires_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_phires_vs_eta_sigma.png has been created
Info in <TCanvas::Print>: png file plots/h_ptres_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_ptres_vs_eta_sigma.png has been created
</verbatim>

---++++++ GCC directly
 To see what is really going on, I provided a simple script that contains a single line to compile this code as a stand alone program (rather than running in ROOT/CINT). The main reason is to demonstrate that it is possible to compile ROOT objects and classes without the interpreter at all (CINT). You will find the interpreter has limitations and sometimes you may want to go around it. One could easily extend this using GNU Make or even a fully flushed out build system or IDE (Ecplipse, Boost Build, CMake, CMSSW's scram, etc.).

To compile from the command prompt:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwkelley@uaf-7 lesson2]$  ./compile.sh 
g++ -O2 source/TrackingEfficiencyAnalysis.cc source/TRKEFF.cc source/HistTools.cc -o tracking_eff_analysis -pthread -m64 -I/code/osgcode/imacneill/root/05.34.07/include -L/code/osgcode/imacneill/root/05.34.07/lib -lCore -lCint -lRIO -lNet -lHist -lGraf -lGraf3d -lGpad -lTree -lRint -lPostscript -lMatrix -lPhysics -lMathCore -lThread -pthread -lm -ldl -rdynamic -lGenVector -Iinclude
</verbatim>

This produces an executable program called "tracking_eff_analysis".
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson2]$  ./tracking_eff_analysis 
[TrackingEfficiencyAnalysis::ScanChain] finished processing 1000 events
------------------------------
CPU  Time: 1.2
Real Time: 1.2

[TrackingEfficiencyAnalysis::EndJob] printing histograms:
Info in <TCanvas::Print>: png file plots/h_cotthetares_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_cotthetares_vs_eta_sigma.png has been created
Info in <TCanvas::Print>: png file plots/h_d0res_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_d0res_vs_eta_sigma.png has been created
Info in <TCanvas::Print>: png file plots/h_den_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_den_vs_pt.png has been created
Info in <TCanvas::Print>: png file plots/h_dzres_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_dzres_vs_eta_sigma.png has been created
Info in <TCanvas::Print>: png file plots/h_eff_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_eff_vs_pt.png has been created
Info in <TCanvas::Print>: png file plots/h_num_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_num_vs_pt.png has been created
Info in <TCanvas::Print>: png file plots/h_phires_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_phires_vs_eta_sigma.png has been created
Info in <TCanvas::Print>: png file plots/h_ptres_vs_eta.png has been created
Info in <TCanvas::Print>: png file plots/h_ptres_vs_eta_sigma.png has been created
</verbatim>

---+++++ Output plots

The output plots will be put in the plots directory. I've provided a macro called [[https://github.com/kelleyrw/AnalysisTutorial/blob/master/lesson2/macros/FormattedPlots.C][Formated]] to produce a nice version of the residual plots. First you must load the HistTools so I use them in the macro:

<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root [0] .L lib/libHistTools.so 
root [1] .x macros/FormattedPlots.C+
Info in <TUnixSystem::ACLiC>: creating shared library /Users/rwk7t/temp/.rootbuild//Users/rwk7t/Development/newbies/lesson2/./macros/FormattedPlots_C.so
Info in <TCanvas::Print>: pdf file plots/sigma_res.pdf has been created
</verbatim>

This produces formated plots: &lt;/verbatim&gt; <img width="1000" alt="sigma_res.png" src="%ATTACHURLPATH%/sigma_res.png" height="1500" />

---+++++ Homework
   * check out, understand, and run the code from lesson2
   * modify the code to also produce the resolution vs pt. The only change to the selection is since this is an efficiency vs pT, we should reduce the pT threshold to pT &gt; 0.1 !GeV.
   * modify the macros to produce the nicely formatted plots using log scale.

---+++ CMSSW
---++++ Overview of !CMSSW
   * Where is the code? 
      * Repository: https://github.com/cms-sw/cmssw
   * Where is the LXR? 
      * !CMSSW Cross-Reference: http://cmslxr.fnal.gov/lxr/
   * Where is the Dyoxygen? 
      * GUI for code: https://cmssdt.cern.ch/SDT/doxygen/
   * Where is the data? 
      * Data Aggregation System: https://cmsweb.cern.ch/das/
      * Example: https://cmsweb.cern.ch/das/request?view=list&limit=10&instance=cms_dbs_prod_global&input=dataset%3D%2F*DYJetsToLL*%2F*%2F*AODSIM*
      * Can also use the DBS on the command line (after !CMSSW is setup -- see below)
---++++ Selected workbook topics Workbook
   * Some of these topics are out of date -- we'll use what we can and I'll supply more relevant examples if they are too far out of date.
   * workbook: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBook
---+++++ Chapter 2
   * Intro to !CMSSW: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookIntroBasics
   * Discussion on computing model: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookComputingModel
   * Basics of !CMSSW: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookCMSSWFramework
---+++++ Chapter 3
   * Ignore PAT
   * work flow of an analysis: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookAnalysisOverviewIntroduction
   * which release: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookWhichRelease 
      * !CMSSW setup: put this in your .bash_profile
      * <verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
source /code/osgcode/cmssoft/cms/cmsset_default.sh
export SCRAM_ARCH=slc5_amd64_gcc462
</verbatim>
      * for this exercise, use CMSSW_5_3_2_patch4 (this is what we currently use in cms2)
      * setup a release by doing the following:
      * <verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
cmsrel CMSSW_5_3_2_patch4
pushd CMSSW_5_3_2_patch4/src
cmsenv
popd
</verbatim>
      * It is often convenient to create a !CMSSW project with a special name, so that its contents are more easily recognized by you. For example, one could
      * <verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
scramv1 p -n CMSSW_5_3_2_patch4_Tutorial CMSSW CMSSW_5_3_2_patch4
pushd CMSSW_5_3_2_patch4_Tutorial/src
cmsenv
popd
</verbatim>
   * Get a ROOT file. You can find them on the dbs (or DAS) and use xrootd to either open then or copy them: 
      * Find a file that you want do down load
      * <verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
dbsql find file where dataset=/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball/Summer12_DR53X-PU_S10_START53_V7A-v1/AODSIM
</verbatim>
      * Open it in xrootd
      * <verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root root://xrootd.unl.edu//store/mc/Summer12_DR53X/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball/AODSIM/PU_S10_START53_V7A-v1/0000/00037C53-AAD1-E111-B1BE-003048D45F38.root
</verbatim>
      * Or copy it locally
      * <verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
xrdcp root://xrootd.unl.edu//store/mc/Summer12_DR53X/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball/AODSIM/PU_S10_START53_V7A-v1/0000/00037C53-AAD1-E111-B1BE-003048D45F38.root dy.root
</verbatim>
      * If you don't have a grid certificate yet, then use the file I copied locally
      * <verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root /nfs-7/userdata/edm/53X/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball_AODSIM_PU_S10_START53_V7A-v1.root
</verbatim>
   * Open a root tree and play with TTree::Draw(): https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookFWLite 
      * Example EDM ROOT file: /nfs-7/userdata/edm/53X/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball_AODSIM_PU_S10_START53_V7A-v1.root
      * <verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwkelley@uaf-7 lesson3]$  root /nfs-7/userdata/edm/53X/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball_AODSIM_PU_S10_START53_V7A-v1.root
root [0] 
Attaching file /nfs-7/userdata/edm/53X/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball_AODSIM_PU_S10_START53_V7A-v1.root as _file0...
Warning in <TClass::TClass>: no dictionary for class edm::EventAuxiliary is available
Warning in <TClass::TClass>: no dictionary for class edm::Hash<2> is available
Warning in <TClass::TClass>: no dictionary for class edm::EventID is available
Warning in <TClass::TClass>: no dictionary for class edm::Timestamp is available
Warning in <TClass::TClass>: no dictionary for class edm::LuminosityBlockAuxiliary is available
Warning in <TClass::TClass>: no dictionary for class edm::LuminosityBlockID is available
...
Warning in <TClass::TClass>: no dictionary for class pair<unsigned int,string> is available
</verbatim>

What were all these errors? The cause is CINT/ROOT is independent of !CMSSW and thus does not natively understand any of the !CMSSW specific classes (reco::Track, edm::EventAuxiliary, !TrackingParticle, etc.). If you are running fully compiled this is not an issue; however, if you want to look at this file via CINT, you will need to load the dictonaries such that ROOT will understand these classes. Fortunately, this can be accomplished by setting of !CMSSW's FWLite:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
{
    // Set up FW Lite for automatic loading of CMS libraries
    // and data formats.   As you may have other user-defined setup
    // in your rootlogon.C, the CMS setup is executed only if the CMS
    // environment is set up.
    //
    const TString cmsswbase = getenv("CMSSW_BASE");
    if (cmsswbase.Length() > 0) {
        //
        // The CMSSW environment is defined (this is true even for FW Lite)
        // so set up the rest.
        //
        cout << "Loading FW Lite setup." << endl;
        gSystem->Load("libFWCoreFWLite.so");
        AutoLibraryLoader::enable();
        gSystem->Load("libDataFormatsFWLite.so");
        gSystem->Load("libDataFormatsPatCandidates.so");
    }
}
</verbatim>

Before you load the file, load FWLite. Now the warnings should disapear.

<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwkelley@uaf-7 lesson3]$  root /nfs-7/userdata/edm/53X/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball_AODSIM_PU_S10_START53_V7A-v1.root
[rwkelley@uaf-7 lesson3]$  root -b
root [0] .x macros/load_fwlite.C 
Loading CMSSW FWLite
root [1] TFile* file = TFile::Open("/nfs-7/userdata/edm/53X/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball_AODSIM_PU_S10_START53_V7A-v1.root")
root [2] 
</verbatim>

Often, people put this into their .rootlogon.C file such that it is loaded with ROOT whenever !CMSSW is setup.

   * Working with real data (JSON's etc...): https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookCollisionsDataAnalysis
---+++++ Chapter 4
   * Basics of analysis: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookMoreOnCMSSWFramework
   * Write an EDAnalyzer: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookWriteFrameworkModule 
      * Implement this module using CMSSW_5_3_2_patch4 (this is what we use for CMS2)
      * DY MC file: /nfs-7/userdata/edm/53X/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball_AODSIM_PU_S10_START53_V7A-v1.root
   * Python configuration: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookConfigFileIntro
   * Explain what an EDFilter and EDProducer does (do an example?)
---+++++ CMSSW Z Analysis exercise
   * Write a EDProducer to make a simple ntuple with some basic variables for a quick generator study of the Z-boson 
      * keep the p4 of the Z and it's daughter leptons 
         * gen label --&gt; "genParticles"
      * match daughter leptons to its corresponding reco objects (if they exist). 
         * muon label --&gt; "muons"
         * electron label --&gt; "gsfElectrons"
   * Write an EDFilter to filter out Z --&gt; tau tau events
   * Write an EDAnalyzer to make some histograms of the variables quantities above 
      * Try to look at the difference between the generator level quantity vs the reco quantity to get a feel for the resolution to measure these quantities (e.g. m_Z vs m_l1l2).
   * I'd like to see the following plots at least: 
      * mass of the Z
      * Compare the dilepton invariant mass at gen level and reco level 
         * for ee, &mu;&mu; and both together
      * Compare p<sub>T</sub> and &eta; of the leptons (gen and reco level)
      * Resolution of the z mass vs z mass
      * Resolution of the lepton p<sub>T</sub> vs p<sub>T</sub> (e's and &mu;'s separately).
      * Efficiency of reconstructing the lepton vs p<sub>T</sub> (e's and &mu;'s separately).
   * Please make a few slides summarizing your results. I will look at them and check your code on Friday.

---+++ Building Lesson
 C was developed in the 1970s and C++ in the 1980s. These both predate nice GUI Integrated Development Environments (IDEs) that are quite popular today. These include MS Visual Studio, Eclipse, XCode, etc. NOTE: the IDE is not the compiler but is merely a nice GUI around the compiler. Most of the time, the nitty gritty details of managing compiling and linking code is handled by the IDE but not always. For large systems, even in industry which uses IDEs, the build system is handled separately from the IDE. Here build refers to the process of compiling and linking your code into the final product (exe, libs, etc.).

In CMS, we use [[https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookScramV1Intro][SCRAM]] (Source Configuration, Release, And Management tool). This short lesson, motivates the need for a "build" system and then goes into how SCRAM solves the problem for CMSSW.
---++++ Why do we care?
 C++ is object oriented. This encourages modular code that can be spread among multiple files. Unfortunately, nobody updated the old file management system from C to handle this so C++ is stuck with quite the antiquated system -- very crude compared to more recently developed languages. The next subsections illustrates some of the issues. 
---+++++ Simple one file program
 Consider the following simple program in a file called =hello.cpp= in the =src= directory. <verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#include <iostream>

int main()
{
    std::cout << "hello world!" << std::endl;
    return 0;
}
</verbatim>

Use g++ to compile
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$ g++ src/hello.cpp 
</verbatim>

This produces an exe called =a.out=. To run:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  ./a.out 
hello world!
</verbatim>

You can control the name of the output exe with the option -o to g++. Here we put the exe in the =bin= directory:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  g++ src/hello.cpp -o bin/hello
[rwk7t@tensor lesson4]$  ./bin/hello 
hello world!
</verbatim>

---+++++ Multiple File programs

Now what happens if we want to write some reusable functions and put them in separate files. How does this change our building procedure? Consider the function =Module1::PrintHello= in a separate file called =module1.cpp=:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#include <iostream>

namespace Module1
{
    void PrintHello()
    {
        std::cout << "hello from module1!" << std::endl;
    }
}
</verbatim>

And the main program that goes with it is called =test1.cpp=:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#include "module1.cpp"

int main()
{
    Module1::PrintHello();
    return 0;
}
</verbatim>

Now we compile into and exe in the =bin= directory:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  g++ src/test1.cpp -o bin/test1
[rwk7t@tensor lesson4]$  ./bin/test1 
hello from module1!
</verbatim>

Everything here looks fine. Now, consider another function added into our program which uses the =Module1::PrintHello= function from =module1.cpp= -- call this =module2.cpp=:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#include <iostream>
#include "module1.cpp" 

namespace Module2
{
    void PrintHello()
    {
        std::cout << "hello from module2!" << std::endl;
        Module1::PrintHello();
    }
}
</verbatim>

Now our main program changes to the following which is saved in =test2.cpp=:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#include "module1.cpp"
#include "module2.cpp"

int main()
{
    Module1::PrintHello();
    Module2::PrintHello();
    return 0;
}
</verbatim>

Now compiling will give you an error:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  g++ src/test2.cpp -o bin/test2
In file included from src/module2.cpp:2:0,
                 from src/test2.cpp:2:
src/module1.cpp: In function 'void Module1::PrintHello()':
src/module1.cpp:5:10: error: redefinition of 'void Module1::PrintHello()'
In file included from src/test2.cpp:1:0:
src/module1.cpp:5:10: error: 'void Module1::PrintHello()' previously defined here
</verbatim>

The reason for the error is module1.cpp has been included twice. Once in the test2.cpp and once in the module2.cpp. Recall from C++ that you can *declare* a function or class many times but you can only *define* it once. Here because we are including the definition twice, we get a compile error.

To fix the problem, we forward declare the two functions in =test3.cpp=:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
namespace Module1 {void PrintHello();}
namespace Module2 {void PrintHello();}

int main()
{
    Module1::PrintHello();
    Module2::PrintHello();
    return 0;
}
</verbatim>

We also need to foward declare the function in "module2.cpp"
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#include <iostream>

namespace Module1 {void PrintHello();}
namespace Module2
{
    void PrintHello()
    {
        std::cout << "hello from module2!" << std::endl;
        Module1::PrintHello();
    }
}
</verbatim>

And when we compile, we need to supply the definitions to the compiler. This is done by adding all three files to the g++ call and now g++ knows to compile all 3 files and then link them together:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  g++ src/test3.cpp src/module1.cpp src/module2.cpp -o bin/test3
[rwk7t@tensor lesson4]$  ./bin/test3 
hello from module1!
hello from module2!
hello from module1!
</verbatim>

Now, editing the files and forward declaring is quite cumbersome so the convention is to factor out forward declarations into interface or "header files" usually denoted by =*.h= or =*.hpp=. So refactoring the modules and main program:

module1.h:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#ifndef MODULE1_H
#define MODULE1_H

namespace Module1
{
    void PrintHello();
}

#endif //MODULE1_H
</verbatim>

Notice that the header file had the code surrounded some C preprocessor commands (called "header guard"). This really only needed for class definitions since these are typically *defined* in header files. This is protect from multiple class definitions. In this particular example, you wouldn't need them but its good practice.

module1.cpp:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#include "module1.h"
#include <iostream>

namespace Module1
{
    void PrintHello()
    {
        std::cout << "hello from module1!" << std::endl;
    }
}
</verbatim>

module2.h:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#ifndef MODULE2_H
#define MODULE2_H

namespace Module2
{
    void PrintHello();
}

#endif //MODULE2_H
</verbatim>

module2.cpp:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#include "module2.h" 
#include "module1.h" 
#include <iostream>

namespace Module2
{
    void PrintHello()
    {
        std::cout << "hello from module2!" << std::endl;
        Module1::PrintHello();
    }
}
</verbatim>

test4.cpp:
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#include "module1.h"
#include "module2.h"

int main()
{
    Module1::PrintHello();
    Module2::PrintHello();
    return 0;
}
</verbatim>

It is customary to put the header files in a different folder than the source code. This is to facilitate a "user interface" that is easy to parse and not polluted with implementation details. This is merely a convention but it is standard practice. Because the headers in this example live in the =include= director, we need to tell g++ where to find them. This is done with the =-I= switch to g++:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  g++ src/test4.cpp -I include src/module1.cpp src/module2.cpp -o bin/test4
[rwk7t@tensor lesson4]$  ./bin/test4 
hello from module1!
hello from module2!
hello from module1!
</verbatim>

---+++++ Libraries

In the previous section, we saw how to build an exe that has code spread among multiple file. What happens though when you have 10, 100, 10k file? You cannot possibly compile every file every time you wish to build your code! For intermediate and large systems it might take hours to compile. For example, compiling CMSSW takes several hours and has to be scheduled. Even ROOT takes ~30 minutes.

To save time, you could compile each piece at a time and then you will only have to link them which can save some building time:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  g++ -c src/module1.cpp -Iinclude -o obj/module1.o
[rwk7t@tensor lesson4]$  g++ -c src/module2.cpp -Iinclude -o obj/module2.o
[rwk7t@tensor lesson4]$  g++ -c src/test4.cpp -Iinclude -o obj/test4.o
[rwk7t@tensor lesson4]$  g++ obj/test4.o obj/module1.o obj/module2.o -o bin/test4 
</verbatim>

This breaks each file into a binary or "object" file by using the -c option. The binary file does nothing on its own but can be linked against in the last line to create the exe. However, again compiling every file into its own object file can be cumbersome for intermediate to large systems so the convention is to create a reusable library.

---++++++ Static Libraries

A static library is essentially where you collect all of the binary files for set of related code and combine it into one file that can be linked against to create you main program. The main program will essentially pull the binaries that are relevant into the main program and absorb the binary content into the exe file. In the following example, we pull module1 and module2 into a single reusable static library:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  g++ -c src/module1.cpp -Iinclude -o obj/module1.o
[rwk7t@tensor lesson4]$  g++ -c src/module2.cpp -Iinclude -o obj/module2.o
[rwk7t@tensor lesson4]$  ar rvs lib/libmodule.a obj/module1.o obj/module2.o 
ar: creating archive lib/libmodule.a
a - obj/module1.o
a - obj/module2.o
</verbatim>

NOTE: the file extension convention for static libraries is .a for "archive".

Now you can use this static library to use this reusable package:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  g++ -Iinclude src/test4.cpp lib/libmodule.a 
[rwk7t@tensor lesson4]$  ./bin/test4 
hello from module1!
hello from module2!
hello from module1!
</verbatim>

There is an interface that g++ uses to link against libraries. The following is equivalent to the above:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  g++ -Iinclude src/test4.cpp -L lib -l module 
[rwk7t@tensor lesson4]$  ./bin/test4 
hello from module1!
hello from module2!
hello from module1!
</verbatim>

Here =-L= allows you to specify the path to the library and =-l= allows you to specify the actual library you wish to link against. Notice the =-l= argument does not have the extensions or the "lib" prefix to the archive file -- this is implied. This is because you may have a dynamic and static version of the same library and by default, dynamic is preferred. The advantage of this method is you don't have to keep repeating the path the libraries which can make you command really long and unreadable.

---++++++ Dynamic Libraries

Static libraries are great for small or intermediate size systems; however, since the binary code is copied to the exe, this can cause "code bloat" and cause the exe to be very large (~GB). Also, if you use a hierarchical library scheme where some libs are dependent on other libs, then you are duplicating binary code and again causing more bloated binaries. The solution is to use dynamic libraries (also called shared libraries). This allows the exe to find the library at runtime. There is a slight initialization penalty but not really that noticeable with modern computers. The advantage is the exe will be small and you don't have to copy around binaries into other libs or exes. This for systems with "standard" shared libraries installed so only the exe needs to be shipped. However, the down side to shared libraries is now you have to keep track of where the dynamic libraries are and if you wish to run the exe somewhere else (like the grid), you will have to send the appropriate libraries as well if they are not already installed somewhere.

To create a dynamic lib:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$ g++ -shared obj/module*.o -o lib/libmodule.so
[rwk7t@tensor lesson4]$ g++ -I include src/test4.cpp -L lib -l module
[rwk7t@tensor lesson4]$ ./bin/test4 
hello from module1!
hello from module2!
hello from module1!
</verbatim>

NOTE: the file extension convention varies by platform:
   * Windows: *. =dll= for dynamic link library
   * Linux: *. =so= for shared object
   * Mac OSX: *. =dylib= for dynamic library

Finally, exe will look in its same directory or the original place where the dynamic lib was during linking. If you move it to some central place, you will need to ensure that the exe can find it at runtime. You can specify an environment variable called =LD_LIBRARY_PATH= to include the path of the libraries you use frequently and the OS will search there for the shared lib. This is usually for system wide code or systems that the user sets up (like ROOT or CMSSW). Mac OSX also has a environment variable also called =DYLD_LIBRARY_PATH= which unfortunately causes mental friction when using a mac.

This only scratches the service of using libraries.

---+++++ Building an exe with ROOT objects

There is no reason you have to use ROOT interactively though CINT -- it is really just a set of C++ libraries. Consider the following simple program called =with_root.cpp=:

<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#include "TH1F.h"
#include "TFile.h"

int main()
{
    TFile file("file.root", "RECREATE");
    TH1F h1("h1", "h1", 100, -5, 5);
    h1.FillRandom("gaus", 10000);
    h1.Write();
    file.Close();
    return 0;
}
</verbatim>

If I try to compile this, I'll get an error since g++ doesn't know where to find the ROOT headers and libraries:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  g++ src/with_root.cpp -o bin/with_root                               
src/with_root.cpp:1:18: fatal error: TH1F.h: No such file or directory
compilation terminated.
</verbatim>

ROOT ships with a nice command line utility that will give you the appropriate g++ options to pass to your command:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  root-config --cflags --libs
-pthread -std=c++0x -m64 -I/usr/local/cmssw/osx107_amd64_gcc472/cms/cmssw/CMSSW_6_1_2/external/osx107_amd64_gcc472/bin/../../../../../../lcg/root/5.34.03-cms2/include -L/usr/local/cmssw/osx107_amd64_gcc472/cms/cmssw/CMSSW_6_1_2/external/osx107_amd64_gcc472/bin/../../../../../../lcg/root/5.34.03-cms2/lib -lCore -lCint -lRIO -lNet -lHist -lGraf -lGraf3d -lGpad -lTree -lRint -lPostscript -lMatrix -lPhysics -lMathCore -lThread -lpthread -Wl,-rpath,/usr/local/cmssw/osx107_amd64_gcc472/cms/cmssw/CMSSW_6_1_2/external/osx107_amd64_gcc472/bin/../../../../../../lcg/root/5.34.03-cms2/lib -lm -ldl
</verbatim>

So to build a standalone exe with ROOT objects, you can do the following:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  g++ src/with_root.cpp `root-config --cflags --libs` -o bin/with_root 
[rwk7t@tensor lesson4]$  ./bin/with_root 
</verbatim>

The point is you are not stuck with !ACLiC to run ROOT code.

---++++ Build Systems

To facilitate managing the low level details from the last section, programs and tools called "build systems" have been developed. This can range from a simple bash script to something from a GUI (like Visual Studio). Learning a build system can increase your productivity and reduce the mental friction of managing the development, build, test cycle.

---+++++ GNU Make

GNU Make is considered a first step towards a build system. GNU Make is not a build system per se; but a tools that can be used to fashion a build system. This section will not go into any real detail other than to give the most basic example. Make is old technology from the 1970s and many more modern systems are available. It is still widely used as a piece of the build system so it is still worth knowing a bit.

Make has the simple structure which is a series of rules:

<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
target:  dependency1 dependency2 ...
      <tab> command1
      <tab> command2
      ... 
</verbatim>

The goal of the each rule is to create the target. If the target is older than any of its dependencies then the commands are executed. You can then build up a series of interconnect rules to build your code. Consider the following example:

<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
# simple Makefile for test4

# target
all: bin/test4

# rule to make the exe
bin/test4: src/test4.cpp lib/libmodule.so
   g++ -Iinclude src/test4.cpp -Llib -lmodule -o bin/test4

lib/libmodule.so: obj/module1.o obj/module2.o
   g++ -shared obj/module1.o obj/module2.o -o lib/libmodule.so

obj/module1.o: include/module1.h src/module1.cpp
   g++ -c src/module1.cpp -Iinclude -o obj/module1.o

obj/module2.o: include/module2.h src/module2.cpp include/module1.h src/module1.cpp
   g++ -c src/module2.cpp -Iinclude -o obj/module2.o

# clean up
clean:
   if [ -f lib/libmodule.so ]; then rm lib/libmodule.so; fi
   if [ -f bin/test4 ]; then rm bin/test4; fi
   if [ -f obj/module1.o ]; then rm obj/module1.o; fi
   if [ -f obj/module2.o ]; then rm obj/module2.o; fi
   if [ -f obj/test4.o ]; then rm obj/test4.o; fi

# PHONY means these rules are always out of date
.PHONY: all clean
</verbatim>

The ultimate goal is create the =all= rule:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  make all
g++ -c src/module1.cpp -Iinclude -o obj/module1.o
g++ -c src/module2.cpp -Iinclude -o obj/module2.o
g++ -shared obj/module1.o obj/module2.o -o lib/libmodule.so
g++ -Iinclude src/test4.cpp -Llib -lmodule -o bin/test4
</verbatim>

Notice the rules get called in reverse since nothing has been made. Also, not that if you just type make, the rule =all= is implied. So this is convenient since whenever a file is out of date, it will call the appropriate rule and so on until the =all= rule is satisfied.

Also, there is a user defined rule clean which deletes all of the binary and library files:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
[rwk7t@tensor lesson4]$  make clean
if [ -f lib/libmodule.a ]; then rm lib/libmodule.a; fi
if [ -f bin/test4 ]; then rm bin/test4; fi
if [ -f obj/module1.o ]; then rm obj/module1.o; fi
if [ -f obj/module2.o ]; then rm obj/module2.o; fi
if [ -f obj/test4.o ]; then rm obj/test4.o; fi
</verbatim>

This might seem magical but for even intermediate size systems (like an analysis), this can get out of hand and cumbersome. Some more digging into Make's features can alleviate some of this but in the end, there is still quite a bit of work to get make to work well.

See: http://oreilly.com/catalog/make3/book/index.csp

---+++++ !ACLiC

ROOT for interactive work uses an interactive system that ships with CINT called The Automatic Compiler of Libraries for CINT (!ACLiC).
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
root [0] .L macro.C++
</verbatim>

!ACLiC is nice for simple and mostly standalone code. Building larger systems can be troublesome. More on !ACLiC can be found here: http://root.cern.ch/download/doc/ROOTUsersGuideChapters/CINT.pdf.

The main disadvantage of !ACLiC is it has a hard time parsing complex code, especially with templates. Thus the need to use something more robust for complex systems.

---+++++ SCRAM

CMSSW uses SCRAM systems (Source Configuration, Release, And Management tool). The following twikis explain it more detail:
   * Quick start: https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookScramV1Intro
   * About buildfiles: 
      * <span style="background-color: transparent; color: blue; text-decoration: underline">https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookBuildFilesIntro</span>
      * <span style="background-color: transparent">%BLUE%<u>https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideBuildFile</u>%ENDCOLOR%</span>
      * <span style="background-color: transparent">More in depth info: </span><span style="background-color: transparent">https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideScram</span>

---+++++ SCRAM !BuildFiles -- Short Tutorial

SCRAM is a build system that is a wrapper around make. Recall that that CMSSW is broken into =Subsystem/package= (e.g. Reconstruction/TrackReco or Demo/DemoZAnalysis). This is historical from the ORCA days (the predecessor to CMSSW). To first order, each package will become a shared library (*.so) and it will live in the $CMSSW_BASE/lib/$SCRAM_ARCH directory (e.g. on my mac -- CMSSW_6_1_2/lib/osx107_amd64_gcc472/).

<verbatim>
   Subsytem1 --> package 1 --> BuildFile.xml --> libSubsystem1Package1.so
             --> package 2 --> BuildFile.xml --> libSubsystem1Package2.so
             --> package 3 --> BuildFile.xml --> libSubsystem1Package3.so
           ....
   Subsytem2 --> package 1 --> BuildFile.xml --> libSubsystem2Package1.so
           ....
</verbatim>

You can do more fancy things like build a standalone executable or plugin. This is beyond the scope of this tutorial -- see the twiki's if you want to go there. I've done some of this and its not terrible.

---++++++ EDM Plugin

To use SCRAM, you present to the system the =BuildFile.xml= (or =CMS.BuildFile=) which is a declarative statement about which libraries you need to include to build this package. Consider the following example of an analysis:

<verbatim>
   Analysis          C++ class           Subystem/Package they live in
      |-------> reco::GenParticles --> DataFormats/HepMCCandidate 
              --> reco::Muons        --> DataFormats/MuonReco
              --> reco::GsfElectrons --> DataFormats/EGammaReco
              --> edm::EDAnalyzer    --> FWCore/Framework
              --> edm::Event         --> FWCore/Framework
              --> edm::ParameterSet  --> FWCore/ParameterSet
              --> TH1D               --> ROOT
              --> TFileService       --> PhysicsTools/UtilAlgos
              ....
</verbatim>

To properly compile and *LINK* this analysis package, consider the following !BuildFile.xml in the $CMSSW_BASE/src/Ryan/Analysis":

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
<use name="FWCore/Framework"/>
<use name="FWCore/PluginManager"/>
<use name="FWCore/ParameterSet"/>
<use name="DataFormats/HepMCCandidate"/>
<use name="DataFormats/MuonReco"/>
<use name="DataFormats/EgammaReco"/>
<use name="PhysicsTools/UtilAlgos"/>
<use name="root"/>
<flags EDM_PLUGIN="1"/>
</verbatim>

This !BuildFile declares library called libRyanAnalysis.so, which is also an EDM plugin that cmsRun will understand (EDProducer, EDAnalyzer, EDFilter, ...). This is registered by the plugin manager at compile time.

   * The lines with =&lt;use name="Subsystem/package"&gt;= means to include and link against this library
   * The lines with =&lt;flags EDM_PLUGIN="1"/&gt;= means to promote this library to a edm plugin
   * The "1" means to use the directory structure to resolve the Subsystem/Package (i.e. "Subsystem/Package") 
      * you could have named it something different but this is conventional.

So when you run cmsRun, the plugin can be used in the python configuration, loaded at runtime and do whatever the plugin was designed to do.

These dependencies change as you write more or change your code. It takes some experience to build up knowledge of where all the different packages are in CMSSW -- the LXR and DOxygen are the best places to figure this out.

---++++++ Reusable Library

Not all the code in CMSSW is an EDM plugin. Sometimes, you want to just make a reusable library that other packages use (e.g. data formats or analysis tools). This is simple in SCRAM. You just don't declare a EDM_PLUGIN and "export" the lib:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
<use name="rootcore"/>
<use name="boost"/>
<use name="gsl"/>
<use name="AnalysisTools/LanguageTools"/>
<export>
  <lib name="1"/>
  <use name="root"/>
</export>
</verbatim>

Here I include the packages that are needed and export the library name. Again, the "1" for lib name means to use the directory structure to resolve the Subsystem/Package (i.e. "Subsystem/Package"). By exporting, I'm saying there will be a library available to everyone of name "Subsystem/Package" (e.g. !AnalysisTools/!RootTools).

---++++++ Purpose of Exporting

Consider that we wrote a reusable library called =MyTools= that we want to expose to "clients" in our system. Consider the following awesome drawing:

<img width="800" alt="export_example.png" src="%ATTACHURLPATH%/export_example.png" height="600" />

Here, the implementation (below the squiggly line) of =MyTools= uses three different packages: ROOT, the BOOST C++ libraries and GNU Scientific Library (GSL). The =MyTools= package used these three package in the implementation of the code but not necessarily in the interface. In the interface (above the squiggly line), it only uses ROOT. For example, consider the following pseudo-code from =MyTools=:

!MyAwesomePlotMaker.h
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#ifndef MYAWESOMEPLOTTOOLS_H
#define MYAWESOMEPLOTTOOLS_H
#include "TH1F.h"

namespace MyTools
{
   TH1F MyAwesomePlotMaker();
}
#endif // MYAWESOMEPLOTTOOLS_H
</verbatim>

!MyAwesomePlotMaker.cc
<verbatim style="padding: 10px; background-color: #add8e6; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#include "MyAwesomePlotMaker.h"
#include "TH1F.h"
#include "boost/shared_ptr.hpp"
#include "gsl/gsl_matrix.h"

namespace MyTools
{
   TH1F MyAwesomePlotMaker()
   {
      // awesome code that does everything
      ...
   }
}
</verbatim>

To properly link against these, the following lines are needed in the !BuildFile.xml:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
<use name="rootcore"/>
<use name="boost"/>
<use name="gsl"/>
</verbatim>

However, the user or "client" of the =MyTools= package, does not need to know about Boost and GSL since these are now binaries are linked dynamically to =MyTools=. However, whenever you use something from another package in the interface (header files) of your package, you will need to tell the client about it. For example, if in my tools I included

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
#include "TH1F.h"
</verbatim>

Now, the client of =MyTools= has to also know about ROOT. If he just includes the =MyTools= package, he will get a linker error. To fix this he needs to explicitly include ROOT as a dependency in the client's !BuildFile.xml. This is *not* sustainable since the client would have to figure out that he needed to include ROOT and whenever you use =MyTools=. This is error prone and inelegant since what if =MyTools= adds another dependency --&gt; if there are 1000 "clients" of =MyTools=, are you going to change the 1000 build files that included =MyTools=?

The way SCRAM deals with this is using "export". When I export my lib, I also export the dependencies that the client needs to link this lib:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
<export>
  <lib name="1"/>
  <use name="root"/>
</export>
</verbatim>

Here, I exposing my library called =Subsystem/package= and also whenever you use this library, you will also get ROOT with it. So if the interface to =MyTools= changes and you need another package in the interface, you just have to change the !BuildFile.xml for that package. So now, the picture of the build decencies will look like:

<img width="800" alt="export_example2.png" src="%ATTACHURLPATH%/export_example2.png" height="600" />

Client now gets the ROOT dependency implicitly. The final !BuildFile.xml to realize this picture will look like:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
<use name="rootcore"/>
<use name="boost"/>
<use name="gsl"/>
<export>
  <use name="root"/>
  <lib name="1"/>
</export>
</verbatim>

Unfortunately, in practice, people are not aware and/or disciplined enough to use the export properly. But you should be aware and try to use this feature. This is also standard practice in commercial C++ systems.

FYI, in my opinion, the =BOOST build= system handle this problem particularly well and is not tied to CMSSW...

---+++++ Other systems

There are other systems that can be used as a build system. One goal of a modern build system is to be "platform independent" -- i.e. not depend on the system that the code is compiled on (Windows, Linux, PS3, iOS, etc.). Examples of build systems nclude but are not limited to:
   * CMake: http://www.cmake.org/
   * BOOST Build (my personal favorite): http://www.boost.org/boost-build2/
   * The system that ships with IDEs: Visual Studio, Eclipse, Code Blocks, XCode

Check out the wiki page on this: http://en.wikipedia.org/wiki/List_of_build_automation_software

---+++ CMS2

CMS2 is the name of the ntupling and supporting framework used by the UCSD/UCSB/FNAL group (a.k.a TAS or SNT). The work flow is to produce, for the whole group, a set of standard CMS2 style ntuples using CMSSW. The ntuples are analysis ready with the relevant variables needed to get started on a physics analysis but are general enough to be flexible. An individual analysis (e.g. Same-Sign dileptons) would take these ntuples and make further selections to produce the final analysis.

---++++ Where is the CMS2 code?

The code is now kept in github: https://github.com/cmstas

---++++ How do I produce CMS2 ntuples?

The CMS2 ntuples are produced via cmsRun from CMSSW. The instructions are kept on the SNT twiki: http://www.t2.ucsd.edu/tastwiki/bin/view/CMS/NtupleMakingInstructions

---++++ Where are they kept?

The ntuples are produced by various members of the group (usually students/postdocs ;) ) and are kept in a central place:
   * MC ntuples: http://www.t2.ucsd.edu/tastwiki/bin/view/CMS/WebHome#Monte_Carlo
   * Data ntuples: http://www.t2.ucsd.edu/tastwiki/bin/view/CMS/WebHome#8_TeV_Data
If you are charged to create ntuples, please ensure that this twiki is up to date with *ALL* the field filled in. If you don't know the value to put in a field -- ask someone!

---++++ Example CMS2 Looper

This section outlines how to checkout and compile the example CMS2 looper. Note that there is no "standard" way to build this code in this group -- the user is on his own to figure it out. This example uses SCRAM since it is supported by the CMS collaboration and you can get help via hypernews. If you decide to use something else (!ACLiC, make, cmake, boost build, IDE, etc.), then you are on your own.

First decide where to put the code in your file system. This example will assume that you created an directory called =cms2_project_example= folder.

Create a base level project directory:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
mkdir cms2_project_example
cd cms2_project_example
</verbatim>

Check out the AnalysisTutorial code:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
git clone https://github.com/kelleyrw/AnalysisTutorial.git 
</verbatim>

Create a CMSSW release area and setup the environment:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
scramv1 p -n CMSSW_5_3_2_patch4_cms2example CMSSW CMSSW_5_3_2_patch4
cd CMSSW_5_3_2_patch4_cms2example/src
cmsenv
</verbatim>

Checkout and compile the CMS2 CORE and Dictionaries:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
git clone https://github.com/cmstas/Dictionaries.git CMS2/Dictionaries
git clone https://github.com/cmstas/CORE.git CMS2/NtupleMacrosCore
cd CMS2/NtupleMacrosCore
git pull
git checkout scram_compliant
./setup/setupCoreForSCRAM.sh setup/cms2_ntuple_postprocess_05.03.23.root 
cd $CMSSW_BASE/src
</verbatim>

CORE is the setup of common code the group shares to ensure reproducibility. It is mainly for implementing selections and corrections to physics objects (e.g. JEC). Also, to facilitate easy access to the ntuples's branches in the looper code, CMS2.h/cc are automatically generated. CORE uses CMS2.h heavily.

Note: CMS2.cc is actually quite slow to compile since it is ~33k lines long, thus the reason for it being made its own library.

Copy the Analysis code over to the CMSSW area (For your code, depending on how you set it up, you could just check it out directly).

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
cp -r ../../AnalysisTutorial/cms2_examples/CMS2Project/* .
</verbatim>

Compile the code one last time to make sure all the modules are compiled:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
scram b -j20
</verbatim>

There should be three directories in your analysis area:
   1 CMS2: this the CMS2 code 
      * Dictionaries: class definitions so that ROOT can understand the non-ROOT native CMS2 types.
      * !NtupleMacrosCore: the "sandbox" for the CORE common code
      * !NtupleMacrosHeader: the "sandbox" for the CMS2.h/cc that was produced by the setup script from CORE.
   1 Packages: this is a "starter" reusable toolkit. Feel free to run with this... 
      * !HistTools: A set of basic ROOT histogram helper functions and classes.
      * !LooperTools: A set of basic tools for writing a looper (not necessarily CMS2 specific)
      * ... add whatever other packages you feel you need...
   1 Analysis: for the analysis specific code 
      * !ExampleCMS2Looper: the actual example looper
      * ...add as many analyses as you need...

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
.
|____CMS2
| |____Dictionaries
| |____NtupleMacrosCore
| |____NtupleMacrosHeader
|____Packages
| |____HistTools
| |____LooperTools
|____Analysis
| |____ExampleCMS2Looper
</verbatim>

There two examples of running the looper. Interactively use the ROOT non-compiled script:

<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
cd Analysis/ExampleCMS2Looper
root -b -q -l macros/run_sample.C 
</verbatim>

Also, there is an example of a fully compiled executable which is defined in bin/run_sample.cc:
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
run_sample
</verbatim>

---+++ A full analysis example -- measuring the Z cross section

Goal measure the &sigma;(pp &rightarrow; Z) x BR(Z &rightarrow; ee/&mu;&mu;) for a subset of the 8 !TeV data.  We will use MC for all backgrounds and ignore details such as scale factors.

---++++ References 

   * Z &rightarrow; ee/&mu;&mu; analysis note for 8 !TeV Data [[%ATTACHURL%/AN2012_067_v6.pdf][AN2012_067_v6]]
      * read sections 1-5, 8, 10
      * The actual data yields are obscurely in the appendix: tables 44 and 45
   * Some old but still useful references:
      * Particle Flow Algorithms:
         * http://cms-physics.web.cern.ch/cms-physics/public/PFT-09-001-pas.pdf
         * https://cds.cern.ch/record/1279341/files/PFT-10-002-pas.pdf
         * https://cds.cern.ch/record/1279347/files/PFT-10-003-pas.pdf
      * Electron Reconstruction and Identification:
         * https://twiki.cern.ch/twiki/bin/viewauth/CMS/EgammaPOG
         * https://twiki.cern.ch/twiki/bin/viewauth/CMS/EgIdentification
         * https://twiki.cern.ch/twiki/bin/viewauth/CMS/ElectronRecoPrinciples
         * [[%ATTACHURL%/NOTE2006_040.pdf][NOTE2006_040.pdf]]: Electron Reco and ID
      * Electron Isolation:
         * https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideEgammaIsolation
         * https://twiki.cern.ch/twiki/bin/viewauth/CMS/EgammaPFBasedIsolation
         * https://twiki.cern.ch/twiki/bin/viewauth/CMS/EgammaEARhoCorrection
      * Muon Reconstruction and Identification:
         * https://twiki.cern.ch/twiki/bin/viewauth/CMS/MuonPOG
         * https://twiki.cern.ch/twiki/bin/viewauth/CMS/MuonReferenceEffs
         * https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideMuonId
         * [[%ATTACHURL%/AN2008_098_v1.pdf][AN2008_098_v1.pdf]]: Muon ID
         * [[%ATTACHURL%/MuonReco.pdf][MuonReco.pdf]]: Muon Reconstruction
      * Muon Isolation:
         * https://twiki.cern.ch/twiki/bin/viewauth/CMS/MuonIsolation

---++++ Datasets for Exercise 

   * We will use 8 !TeV dataset run2012A 06Aug2012 (82 pb<sup>-1</sup>).  This is small enough to finish interactively:
      * data: http://www.t2.ucsd.edu/tastwiki/bin/view/CMS/2012Data8TeV_slim_Winter13#Run2012A_06Aug2012_v1_series_run

Single muon data 
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Data2012/CMSSW_5_3_2_patch4_V05-03-24/SingleMu_Run2012A-recover-06Aug2012-v1_AOD/merged/*.root");
</verbatim>

Single electron data 
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Data2012/CMSSW_5_3_2_patch4_V05-03-24/SingleElectron_Run2012A-recover-06Aug2012-v1_AOD/merged/*.root");
</verbatim>

      * Good run list:
         * [[%ATTACHURL%/Cert_190782-190949_8TeV_06Aug2012ReReco_Collisions12_cms2.txt][Cert_190782-190949_8TeV_06Aug2012ReReco_Collisions12_cms2.txt]]: Flat CMS2 friendly JSON
         * [[%ATTACHURL%/Cert_190782-190949_8TeV_06Aug2012ReReco_Collisions12.json][Cert_190782-190949_8TeV_06Aug2012ReReco_Collisions12.json]]: JSON file
         * [[%ATTACHURL%/Cert_190782-190949_8TeV_06Aug2012ReReco_Collisions12.lumi][Cert_190782-190949_8TeV_06Aug2012ReReco_Collisions12.lumi]]: Luminosity measurement

   * mc: http://www.t2.ucsd.edu/tastwiki/bin/view/CMS/Summer12MonteCarlo53X_Slim_Winter13
      * We are using only a subset of the full dataset since this is an exercise and we want this to run interactively

Drell-Yan
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Summer12_53X_MC/DYJetsToLL_M-50_TuneZ2Star_8TeV-madgraph-tarball_Summer12_DR53X-PU_S10_START53_V7A-v1/V05-03-23/merged_ntuple_1[0-9].root");
</verbatim>

W + Jets &rightarrow; l + &nu;
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Summer12_53X_MC/WJetsToLNu_TuneZ2Star_8TeV-madgraph-tarball_Summer12_DR53X-PU_S10_START53_V7A-v1/V05-03-28/merged_ntuple_1[0-9].root");
</verbatim>

tt(bar) &rightarrow; 2l + 2&nu; 
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Summer12_53X_MC/TTJets_FullLeptMGDecays_8TeV-madgraph_Summer12_DR53X-PU_S10_START53_V7A-v2/V05-03-24/merged_ntuple_1[0-9].root");
</verbatim>

tt(bar) &rightarrow; l + &nu; + jj 
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Summer12_53X_MC/TTJets_SemiLeptMGDecays_8TeV-madgraph_Summer12_DR53X-PU_S10_START53_V7A_ext-v1/V05-03-24/merged_ntuple_1[0-9].root");
</verbatim>

tt(bar) &rightarrow; hadronic 
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Summer12_53X_MC/TTJets_HadronicMGDecays_8TeV-madgraph_Summer12_DR53X-PU_S10_START53_V7A_ext-v1/V05-03-24/merged_ntuple_1[0-9].root");
</verbatim>

QCD muon enriched (use for &mu;&mu; analysis).  For now, let's ignore QCD for ee analysis since it requires more statistics than is practical without using the grid.
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Summer12_53X_MC/QCD_Pt_20_MuEnrichedPt_15_TuneZ2star_8TeV_pythia6_Summer12_DR53X-PU_S10_START53_V7A-v3/V05-03-18_slim/merged_ntuple_1[0-9].root");
</verbatim>

WW &rightarrow; 2l + 2&nu;
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Summer12_53X_MC/WWJetsTo2L2Nu_TuneZ2star_8TeV-madgraph-tauola_Summer12_DR53X-PU_S10_START53_V7A-v1/V05-03-23/merged_ntuple_1[0-9].root");
</verbatim>

WZ &rightarrow; 2l + 2q
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Summer12_53X_MC/WZJetsTo2L2Q_TuneZ2star_8TeV-madgraph-tauola_Summer12_DR53X-PU_S10_START53_V7A-v1/V05-03-23/merged_ntuple_1[0-9].root");
</verbatim>

WZ &rightarrow; 3l + &nu;
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Summer12_53X_MC/WZJetsTo3LNu_TuneZ2_8TeV-madgraph-tauola_Summer12_DR53X-PU_S10_START53_V7A-v1/V05-03-23/merged_ntuple_1[0-9].root");
</verbatim>

ZZ &rightarrow; 2l + 2&nu;
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Summer12_53X_MC/ZZJetsTo2L2Nu_TuneZ2star_8TeV-madgraph-tauola_Summer12_DR53X-PU_S10_START53_V7A-v3/V05-03-23/merged_ntuple_1[0-9].root");
</verbatim>

ZZ &rightarrow; 2l + 2q
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Summer12_53X_MC/ZZJetsTo2L2Q_TuneZ2star_8TeV-madgraph-tauola_Summer12_DR53X-PU_S10_START53_V7A-v1/V05-03-23/merged_ntuple_1[0-9].root");
</verbatim>

ZZ &rightarrow; 4l
<verbatim style="padding: 10px; background-color: #d3d3d3; text-align: left; margin-left: 20px; margin-right: 20px; color: black">
    TChain chain("Events");
    chain.Add("/hadoop/cms/store/group/snt/papers2012/Summer12_53X_MC/ZZJetsTo2L2Nu_TuneZ2star_8TeV-madgraph-tauola_Summer12_DR53X-PU_S10_START53_V7A-v3/V05-03-23/merged_ntuple_1[0-9].root");
</verbatim>

---++++ Deliverables
   1 Due Friday 3/14: Set of slides to summarize their understanding of what we went over.  Make sure you cover the following:
      * How is a cross section measured --> what are the pieces?
      * What is the signal?
      * What is a "prompt" lepton?
      * What are the major backgrounds for this analysis?  Why are they backgrounds?
   1 Due Friday 3/14: A spreadsheet of the expected number of events for the signal and the background
      * Cross Sections can be found here: https://twiki.cern.ch/twiki/bin/viewauth/CMS/StandardModelCrossSectionsat8TeV
         * PDF print out if you don't have CERN access yet: [[%ATTACHURL%/StandardModelCrossSectionsat8TeVTwiki.pdf][StandardModelCrossSectionsat8TeVTwiki.pdf]]
      * An example spreadsheet can be found here: [[%ATTACHURL%/ZAnalysis_Expected_Events.xlsx][ZAnalysis_Expected_Events.xlsx]]
   1 Due: Friday 4/11: m_{ll} plots and table of yields:
      * A total of 5 plots -- m_{ee} and m_{&mu;&mu;} plot of
         * gen level with 2 true opposite-signed and same-flavor leptons (OSSF)
         * reco level with opposite-signed and same-flavor leptons (OSSF) -- no gen matching requirement
           * Select all OSSF lepton pairs (no disambiguation yet)
      * data overlaid with a stacked plot of all the backgrounds.
      * Background should be normalized to the lumi*cross-section
         * The gen level plots/yields should have numbers that are consistent with your spreadsheet numbers
   1 Due Wednesday 4/16 or Friday 4/18:  Write up a 20 minute presentation on the e/&mu; ID or Isolation.  
      * ~3/4 of talk should be on: What is it and why do we need it?  What are the variables involved?  
      * ~1/4 of talk should be on: How do I implement it using CMS2 ntuples.
      * schedule:
         * Dan: electron identification (wednesday)
         * Dylan: electron Isolation (wednesday)
         * Mark: muon identification (Friday)
         * Liam: muon isolation (Friday)
   1 Due Friday 4/25: Implement a cut flow
   1 Due Friday 4/2: Final results
   1 Due Friday 5/8: Cross check results using functions from CORE
   1 Due Friday 5/16: Slides of the Results at the level of a "full status report"

-- Main.RyanKelley - 2015/03/31
